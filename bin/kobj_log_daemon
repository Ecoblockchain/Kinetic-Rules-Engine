#!/usr/bin/perl -w

use strict;
use warnings;
use lib qw(
/web/lib/perl
/web/etc
);


use Getopt::Std;
use File::Find::Rule;
use DateTime;
use Sys::Hostname;
use File::Basename;

use Kynetx::Configure qw/:all/;


# configure KNS
Kynetx::Configure::configure();

my $bucket_name = get_config('LOG_BUCKET') || 'k-logs';
my $bucket_prefix = 'rawlogs/';

my $web_root = get_config('WEB_ROOT') || 
    die "WEB_ROOT is undefined in the configuration";

my $log_sink = get_config('LOG_SINK');
my $log_account = get_config('LOG_ACCOUNT') || 'web';

my $days_to_live = "90";

my $GZIP = '/bin/gzip';
my $RSYNC = '/usr/bin/rsync';
my @logdirs = ("$web_root/logs/kynetx");

my $tmpdir = "$web_root/tmp/kobj-logs";
mkdir $tmpdir unless(-e $tmpdir);

# global options
use vars qw/ %opt /;
my $opt_string = 'hm:u:vfa';
getopts( "$opt_string", \%opt ); # or &usage();

my $machine = $opt{'m'} || $log_sink ; #|| 'unionstation.kobj.net';
my $unique = $opt{'u'} || hostname;
my $verbose = $opt{'v'};
my $use_amazon = $opt{'a'};
my $finalize = $opt{'f'} || 0;
&usage() if $opt{'h'};


my $rule = File::Find::Rule->new;

$rule->or(
    $rule->new->directory->name('*.gz')->prune->discard,
    $rule->new->file->name( '*.log' )
);


# get all the files
my @files;
for my $dir ( @logdirs ) {
    print "Looking for log files in $dir\n" if $verbose;
    push( @files, (-d $dir) ? $rule->in($dir) : $dir );
}


# sort files and remove the newest one of each type
#my @newfiles = sort {((stat($a))[10]) cmp ((stat($b))[10])} @files;
my @newfiles = sort {$b cmp $a} @files;
#pop @files unless $finalize;

my %seen;

@files = ();
foreach my $file (@newfiles) {
    my $type = basename $file;
    $type =~ s/^([A-Z]+).*/$1/;
#    print "Seeing file of type $type\n" if $verbose;
#    print "for $file, seen is true" if $seen{$type};
    # don't take the current log file
    if($seen{$type} || $finalize) {
	push(@files, $file);
    } else {
	$seen{$type} = 1;
	next;
    }
}

my ($bucket, $s3);
if ($use_amazon) {
  require Amazon::S3;
  Amazon::S3->import;


  print "Pushing files to S3\n" if $verbose;
  # load the Amazon credentials
  # these are not in the code repository on purpose
  require amazon_credentials; 

  $s3 = Amazon::S3->new(
	{   aws_access_key_id     => amazon_credentials->get_key_id(),
	    aws_secret_access_key => amazon_credentials->get_access_key()
	}
	);

  $bucket = $s3->bucket($bucket_name) or warn $s3->err . ": " . $s3->errstr;;


}

# now process the files
for my $file (@files) {
    print "Processing $file\n" if $verbose;
    system "sudo /bin/chown web $file";
    my $dir = dirname $file;
    my $name = basename $file;
    # new file name has unique machine ID
    my $nf = "$tmpdir/$unique-$name";
    system "sudo /bin/mv $file $nf" || warn "Couldn't move $file: $!";
    print "gzipping $nf\n" if $verbose;
    system  "$GZIP $nf\n";
    if ( $use_amazon ) {
      (my $stem = $name) =~ s/\.log$//;
      my $s3_name = "$bucket_prefix$stem-$unique.log.gz";
      print "Adding $nf.gz to bucket as $s3_name\n" if $verbose;
      $bucket->add_key_filename( 
	$s3_name, # file name to store
	$nf.".gz", # file name to load
	{'content_type' => 'application/gzip'},
        ) or warn $s3->err . ": " . $s3->errstr;

    }
}


my @tmpfiles = glob("$tmpdir/*.gz");
# only run rsync if there's something to transfer
if (int(@tmpfiles)) {
    my $rsync_cmd = 
	"$RSYNC -e ssh --remove-sent-files  " . 
	$tmpdir . "/*.gz " .
	$log_account . "@" . $machine . ":logs";
    print "Running $rsync_cmd\n" if $verbose;
    eval { system $rsync_cmd };

    # if anything's left (i.e. we're not rsyncing)    
    my $rm_cmd = "find $tmpdir  -mtime +$days_to_live -delete";
    system $rm_cmd;
     
}



1;


sub usage {
    print STDERR <<EOF;

usage:  

    kobj_log_daemon -m MACHINE

Moves log files to the "logs" directory of user "web" on the specified 
machine using rsync.  The source files are deleted after they're moved.

Options are:

  -m M	: name of machine to move logs files to (default: $log_sink)
  -u U  : unique identifier for these log files (default: hostname)
  -v    : vebose
  -f    : finalize; don't skip most recent file.  Ensure Web server 
          has been halted before using this switch. 
  -a    : push logs to S3 bucket in addition to transfering to the log sink 

EOF

exit;


}

